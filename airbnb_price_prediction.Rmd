---
title: "Airbnb Price Prediction"
subtitle: Mihai-Bogdan Bîndilă 3264424
output:
  html_document:
    df_print: paged
---

```{r import_packages, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(conflicted)
library(tidyverse)
library(rsample)
library(caret)
library(MASS)
library(randomForest)
```

### Problem description

The number of listings on Airbnb has recorded a [steady increase](https://www.businessofapps.com/data/airbnb-statistics/), from 300,000 in 2013 to 7.7 million in 2023. This project aims to leverage this rich data to identify key factors influencing nightly rental prices. With a focus on Amsterdam as a case study, we will build a predictive model to estimate these prices. The developed model will be integrated into a web application designed for two user groups: 

**Owners:** Gain insights into optimal pricing strategies based on property characteristics and potential guest ratings. \
**Guests:** Assess the fairness of listed prices and explore price variations across different locations for similar properties.

### Data description

It contains the following features that will be used to predict the price per night (realSum): \
**- property characteristics:** type of room, private or shared, capacity, number of bedrooms, for business purposes or not \
**- data from Tripadvisor:** accessibility to attractions and restaurants within the neighborhood of the property \
**- location:** longitude, latitude, distance from the city center and from the nearest metro station \
**- ratings:** cleanliness and guest satisfaction scores \
**- host:** is super host or not


```{r load_data}
data_weekdays <- read_delim(
  file = "./data/amsterdam_weekdays.csv",
  delim = ",",
  col_names = TRUE,
  show_col_types = FALSE
)
data_weekends <- read_delim(
  file = "./data/amsterdam_weekends.csv",
  delim = ",",
  col_names = TRUE,
  show_col_types = FALSE
)
complete_data <- bind_rows(data_weekdays, data_weekends)
write.csv(data, file = "./data/amsterdam_raw.csv", row.names = FALSE)
complete_data
```


### Data preprocessing

```{r factor_categorical_features}
# factor the categorical features
complete_data <- complete_data %>%
  mutate(room_type = factor(room_type)) %>%
  mutate(room_shared = factor(room_shared)) %>%
  mutate(room_private = factor(room_private)) %>%
  mutate(host_is_superhost = factor(host_is_superhost)) %>%
  mutate(multi = factor(multi)) %>%
  mutate(biz = factor(biz))
```

### Data cleaning
- drop features w/o variance or unimportant
- check is there are missing values

```{r data_cleaning}
# remove redundant features
data <- subset(complete_data, select = -c(index, attr_index, rest_index, room_private))

# rename features so that their name is more meaningful
data <- rename(
  data,
  price = realSum,
  business_room = biz,
  is_superhost = host_is_superhost,
  multiple_rooms = multi,
  satisfaction_rating = guest_satisfaction_overall,
  center_dist = dist,
  attractions_index = attr_index_norm,
  restaurants_index = rest_index_norm,
  longitude = lng,
  latitude = lat,
  capacity = person_capacity,
  bedrooms_no = bedrooms
)

# make sure all binary categorical features have the values TRUE and FALSE, instead of numeric values (0 and 1)
data$multiple_rooms <- ifelse(data$multiple_rooms == 0, FALSE, TRUE)
data$business_room <- ifelse(data$business_room == 0, FALSE, TRUE)

# scale the cleanliness rating to have the same scale as the satisfaction rating
data$cleanliness_rating  <- data$cleanliness_rating * 10

data
```

### Data visualization
- check if there are outliers

### Feature engineering
- feature selection
```{r feature_importance}
plot_feature_importance <- function(rf_model) {
  # build the feature importance plot given by the model on the entire dataset
  feature_importance <- as.data.frame(importance(rf_model)) %>%
    rename("importance" = "IncNodePurity")
  feature_importance$importance  <- feature_importance$importance / sum(feature_importance$importance) * 100
  feature_importance <- feature_importance %>% arrange(desc(importance))
  
  ggplot(feature_importance,
         aes(
           x  = rownames(feature_importance),
           y = importance,
           fill = importance
         )) +
    geom_bar(stat = "identity") +
    scale_fill_gradient(low = "#767676", high = "#FF5A5F") +
    labs(title = "Scaled Feature Importance", x = "Feature", y = "Importance") +
    theme_minimal() +
    guides(fill = "none") +
    scale_y_continuous(breaks = seq(0, 100, by = 5),
                       labels = seq(0, 100, by = 5),) +
    scale_x_discrete(limits = rownames(feature_importance)) +
    coord_flip()
}
```


```{r feature_selection, warning=FALSE}
# ensure results are reproducible
set.seed(19)

# set up the control parameters for recursive feature elimination
control <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5,
                      saveDetails = TRUE)

# perform recursive feature elimination
rfe_results <- rfe(
  x = data[, !colnames(data) %in% "price", drop = FALSE],
  y = data[["price"]],
  method = "rf",
  metric = "RMSE",
  sizes = seq(1:ncol(data)),
  rfeControl = control,
  ntree = 100
)

ggplot(data = rfe_results, metric = "RMSE") +
  theme_minimal() +
  labs(title = "Cross-validation RMSE for each best subset of features", x = "Variables", y = "RMSE")

predictors(rfe_results)
rfe_results$results

# create a formula based on the selected features
selected_features_list <- c()

for (feature in colnames(data)) {
  for (important_feature in predictors(rfe_results)) {
    if (grepl(feature, important_feature)) {
      selected_features_list <- append(selected_features_list, feature)
      break
    }
  }
}

# get only the selected features and store the filtered dataset for further use
selected_features_list <- append(selected_features_list, "price")
data <- data[, selected_features_list]
selected_features_list

# retrain the Random Forest with the selected features and then further remove the ones which are not important
rf_selected_features <- randomForest(price ~., data = data, ntree = 100, importance = TRUE)
rf_selected_features
plot_feature_importance(rf_selected_features)
data <- dplyr::select(data, -"is_superhost", -"business_room", -"cleanliness_rating")

# store the final dataset
write.csv(data, file = "./data/amsterdam_preprocessed.csv", row.names = FALSE)
```


### Modeling
- train
- optimize
- validate
- interpret weights / feature importance
- perform statistical tests


```{r modeling_optimization, warning=FALSE}
# tune the hyperparameters
optimization_control <- trainControl(method = 'cv', number = 5)
number_of_features <- ncol(data)

mtry_values <- c(number_of_features,
                 round(sqrt(number_of_features)),
                 round(number_of_features / 3))
ntree_values <- c(64, 100, 128, 500)
best_rmse <- .Machine$integer.max

for (ntree_value in ntree_values) {
    rf_optimized <- train(
      price ~ .,
      data = data,
      method = 'rf',
      metric = 'RMSE',
      ntree = ntree_value,
      tuneGrid = expand.grid(.mtry = mtry_values),
      trControl = optimization_control
    )

    rmse <- min(rf_optimized$results$RMSE)

    if (best_rmse > rmse) {
      best_rmse <- rmse
      best_rf_optimized <- rf_optimized$finalModel
    }
}

best_rmse
best_rf_optimized
```

### Final model

```{r final_model}
# get the best params and re-fit the model on the whole dataset, then save it
rf_final_model <- randomForest(
  price ~.,
  data = data,
  importance = TRUE,
  mtry = best_rf_optimized$mtry,
  ntree = best_rf_optimized$ntree,
)
rf_final_model
plot_feature_importance(rf_final_model)
saveRDS(rf_final_model, file = "./airbnb_price_prediction/rf_model.rds")
```

### Reflection
